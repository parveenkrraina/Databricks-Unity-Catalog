{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5dadc43-b740-4e5d-bc47-74743e50a365",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Programmatically create multiple tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "065c05b3-28ac-42e9-aab2-492e7fc63c2b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You can use Python with Delta Live Tables to programmatically create multiple tables to reduce code redundancy.\n",
    "\n",
    "You might have pipelines containing multiple flows or dataset definitions that differ only by a small number of parameters. This redundancy results in pipelines that are error-prone and difficult to maintain. For example, the following diagram shows the graph of a pipeline that uses a fire department dataset to find neighborhoods with the fastest response times for different categories of emergency calls. In this example, the parallel flows differ by only a few parameters.\n",
    "<img src=\"https://learn.microsoft.com/en-us/azure/databricks/_static/images/workflows/delta-live-tables/fire-dataset-flows.png\"> </src>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33d606af-3352-4000-b64b-d0da1e48ca89",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Delta Live Tables metaprogramming with Python example\n",
    "You can use a metaprogramming pattern to reduce the overhead of generating and maintaining redundant flow definitions. Metaprogramming in Delta Live Tables is done using Python inner functions. Because these functions are lazily evaluated, you can use them to create flows that are identical except for input parameters. Each invocation can include a different set of parameters that controls how each table should be generated, as shown in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da23c7b4-e12c-410c-abf0-cbb17db2484c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06fc089d-9f10-487d-a4c8-2b02e6ef3a56",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Important\n",
    "\n",
    "Because Python functions with Delta Live Tables decorators are invoked lazily, when creating datasets in a loop you must call a separate function to create the datasets to ensure correct parameter values are used. Failing to create datasets in a separate function results in multiple tables that use the parameters from the final execution of the loop.\n",
    "\n",
    "The following example calls the create_table() function inside a loop to create tables t1 and t2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "778c6759-81f4-44b4-bca0-0f4040b19811",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<ol>\n",
    "<li class=\"has-line-data\" data-line-start=\"0\" data-line-end=\"1\">def create_table(name): - This defines a function create_table that takes one argument, name, which is expected to be the name of the table you want to create.</li>\n",
    "<li class=\"has-line-data\" data-line-start=\"1\" data-line-end=\"2\">@dlt.table(name=name) - This is a decorator that applies to the function defined immediately below it. The dlt.table decorator is used to register a function as a Databricks Delta Live Table (DLT). The name=name part sets the name of the DLT to the value of the name parameter passed to the create_table function.</li>\n",
    "<li class=\"has-line-data\" data-line-start=\"2\" data-line-end=\"3\">def t(): - This defines a nested function within create_table, which will be registered as a DLT due to the decorator above it.</li>\n",
    "<li class=\"has-line-data\" data-line-start=\"3\" data-line-end=\"4\">return spark.read.table(name) - Inside the nested function t, it reads a table from the Spark session using the name provided and returns it. This is the operation that will be performed when the DLT is executed.</li>\n",
    "<li class=\"has-line-data\" data-line-start=\"4\" data-line-end=\"5\">tables = [“t1”, “t2”] - This creates a list of table names.</li>\n",
    "<li class=\"has-line-data\" data-line-start=\"5\" data-line-end=\"6\">for t in tables: - This starts a loop over the list of table names.</li>\n",
    "<li class=\"has-line-data\" data-line-start=\"6\" data-line-end=\"9\">create_table(t) - Inside the loop, it calls the create_table function with each table name, which will define and register a new DLT for each table name in the list.<br>\n",
    "In essence, this code is dynamically creating and registering DLTs for each table name specified in the tables list. When executed in a Databricks environment, it would result in the creation of two DLTs named t1 and t2, each associated with a Spark table of the same name.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "174783d3-79ce-4dfe-bb9a-6d0945aa13c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 642, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File \"/databricks/spark/python/dlt/helpers.py\", line 29, in call\n    res = self.func()\n  File \"/root/.ipykernel/1060/command-4315130338077372-363941997\", line 4, in t\n    return spark.read.table(name)\n  File \"/databricks/spark/python/dlt/overrides.py\", line 34, in dlt_read_table_fn\n    return spark_read_table(self, name)\n  File \"/databricks/spark/python/pyspark/instrumentation_utils.py\", line 48, in wrapper\n    res = func(*args, **kwargs)\n  File \"/databricks/spark/python/pyspark/sql/readwriter.py\", line 484, in table\n    return self._df(self._jreader.table(tableName))\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1355, in __call__\n    return_value = get_return_value(\n  File \"/databricks/spark/python/pyspark/errors/exceptions/captured.py\", line 194, in deco\n    raise converted from None\npyspark.errors.exceptions.captured.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `t1` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n'UnresolvedRelation [t1], [], false\n\n\n"
     ]
    }
   ],
   "source": [
    "def create_table(name):\n",
    "  @dlt.table(name=name)\n",
    "  def t():\n",
    "    return spark.read.table(name)\n",
    "\n",
    "tables = [\"t1\", \"t2\"]\n",
    "for t in tables:\n",
    "  create_table(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cb78906-bb53-4ad4-ac14-6ffcc3b2f650",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<ol>\n",
    "<li class=\"has-line-data\" data-line-start=\"0\" data-line-end=\"3\">Import Statements:<br>\n",
    "o   import dlt: This imports the dlt module, which is likely specific to Databricks Delta Live Tables (DLT).<br>\n",
    "o   from pyspark.sql.functions import *: This imports all functions from the pyspark.sql.functions module.</li>\n",
    "<li class=\"has-line-data\" data-line-start=\"3\" data-line-end=\"10\">Creating a Raw Table:<br>\n",
    "o   @dlt.table(name=“raw_fire_department”, comment=“raw table for fire department response”): This decorator defines a DLT named “raw_fire_department” with a comment describing its purpose.<br>\n",
    "o   @dlt.expect_or_drop(“valid_received”, “received IS NOT NULL”), @dlt.expect_or_drop(“valid_response”, “responded IS NOT NULL”), and @dlt.expect_or_drop(“valid_neighborhood”, “neighborhood != ‘None’”): These decorators specify expectations or filters for the data in the table. For example, the first one ensures that the “received” column is not null.<br>\n",
    "o   Inside the decorated function get_raw_fire_department(), the following operations are performed:<br>\n",
    "   Read a CSV file from the specified path (’/databricks-datasets/timeseries/Fires/Fire_Department_Calls_for_Service.csv’).<br>\n",
    "   Rename columns using withColumnRenamed.<br>\n",
    "   Select specific columns: ‘call_type’, ‘received’, ‘responded’, and ‘neighborhood’.</li>\n",
    "<li class=\"has-line-data\" data-line-start=\"10\" data-line-end=\"21\">Generating Tables:<br>\n",
    "o   The generate_tables function creates two DLTs based on the provided parameters:<br>\n",
    "   create_call_table():<br>\n",
    "   Reads data from the “raw_fire_department” DLT.<br>\n",
    "   Filters rows where the “call_type” matches the specified filter (e.g., “Alarms”).<br>\n",
    "   Converts timestamps to Unix timestamps.<br>\n",
    "   Selects the “neighborhood” column.<br>\n",
    "   create_response_table():<br>\n",
    "   Reads data from the previously created DLT (e.g., “alarms_table”).<br>\n",
    "   Calculates the average response time (ts_received - ts_responded) for each neighborhood.<br>\n",
    "   Orders the results by response time and limits to the top 10 neighborhoods.</li>\n",
    "<li class=\"has-line-data\" data-line-start=\"21\" data-line-end=\"24\">Table Names and Appending to all_tables:<br>\n",
    "o   The generate_tables function is called three times with different parameters (“alarms_table”, “fire_table”, and “medical_table”).<br>\n",
    "o   The resulting DLT names (“alarms_response”, “fire_response”, and “medical_response”) are appended to the all_tables list.</li>\n",
    "<li class=\"has-line-data\" data-line-start=\"24\" data-line-end=\"30\">Summary Table:<br>\n",
    "o   A final DLT named “best_neighborhoods” is created.<br>\n",
    "o   It combines data from all previously generated DLTs (union operation).<br>\n",
    "o   Groups by neighborhood and calculates the count (score) for each neighborhood.<br>\n",
    "o   Orders the results by score.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41a82a3b-80bf-46ce-b784-de5b1c42ac93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "<html>\n",
       "  <style>\n",
       "<style>\n",
       "      html {\n",
       "        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,\n",
       "        Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,\n",
       "        Noto Color Emoji,FontAwesome;\n",
       "        font-size: 13;\n",
       "      }\n",
       "\n",
       "      .ansiout {\n",
       "        padding-bottom: 8px;\n",
       "      }\n",
       "\n",
       "      .createPipeline {\n",
       "        background-color: rgb(34, 114, 180);\n",
       "        color: white;\n",
       "        text-decoration: none;\n",
       "        padding: 4px 12px;\n",
       "        border-radius: 4px;\n",
       "        display: inline-block;\n",
       "      }\n",
       "\n",
       "      .createPipeline:hover {\n",
       "        background-color: #195487;\n",
       "      }\n",
       "\n",
       "      .tag {\n",
       "        border: none;\n",
       "        color: rgb(31, 39, 45);\n",
       "        padding: 2px 4px;\n",
       "        font-weight: 600;\n",
       "        background-color: rgba(93, 114, 131, 0.08);\n",
       "        border-radius: 4px;\n",
       "        margin-right: 0;\n",
       "        display: inline-block;\n",
       "        cursor: default;\n",
       "      }\n",
       "\n",
       "      table {\n",
       "        border-collapse: collapse;\n",
       "        font-size: 13px;\n",
       "      }\n",
       "\n",
       "      th {\n",
       "        text-align: left;\n",
       "        background-color: #F2F5F7;\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "      }\n",
       "\n",
       "      tr {\n",
       "        border-bottom: solid;\n",
       "        border-bottom-color: #CDDAE5;\n",
       "        border-bottom-width: 1px;\n",
       "      }\n",
       "\n",
       "      td {\n",
       "        padding-left: 8px;\n",
       "        padding-right: 8px;\n",
       "      }\n",
       "\n",
       "      .dlt-label {\n",
       "        font-weight: bold;\n",
       "      }\n",
       "\n",
       "      ul {\n",
       "        list-style: circle;\n",
       "        padding-inline-start: 12px;\n",
       "      }\n",
       "\n",
       "      li {\n",
       "        padding-bottom: 4px;\n",
       "      }\n",
       "</style></style>\n",
       "  \n",
       "<div class=\"ansiout\">\n",
       "<span class='tag'>best_neighborhoods</span> is defined as a\n",
       "<span class=\"dlt-label\">Delta Live Tables</span> dataset\n",
       " with schema: \n",
       "</div>\n",
       "\n",
       "  \n",
       "<div class=\"ansiout\">\n",
       "   <table>\n",
       "     <tbody>\n",
       "       <tr>\n",
       "         <th>Name</th>\n",
       "         <th>Type</th>\n",
       "       </tr>\n",
       "       \n",
       "<tr>\n",
       "   <td>neighborhood</td>\n",
       "   <td>string</td>\n",
       "</tr>\n",
       "\n",
       "<tr>\n",
       "   <td>score</td>\n",
       "   <td>bigint</td>\n",
       "</tr>\n",
       "     </tbody>\n",
       "   </table>\n",
       "</div>\n",
       "\n",
       "  <div class =\"ansiout\">\n",
       "    To populate your table you must either:\n",
       "    <ul>\n",
       "      <li>\n",
       "        Run an existing pipeline using the\n",
       "        <span class=\"dlt-label\">Delta Live Tables</span> menu\n",
       "      </li>\n",
       "      <li>\n",
       "        Create a new pipeline: <a class='createPipeline' href=\"?o=2327154142455768#joblist/pipelines/create?initialSource=%2FUsers%2Fparveen.raina%40vnodeites.com%2FProgrammatically-create-multiple-tables&redirectNotebookId=4315130338077367\">Create Pipeline</a>\n",
       "      </li>\n",
       "    </ul>\n",
       "  <div>\n",
       "</html>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "@dlt.table(\n",
    "  name=\"raw_fire_department\",\n",
    "  comment=\"raw table for fire department response\"\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_received\", \"received IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_response\", \"responded IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_neighborhood\", \"neighborhood != 'None'\")\n",
    "def get_raw_fire_department():\n",
    "  return (\n",
    "    spark.read.format('csv')\n",
    "      .option('header', 'true')\n",
    "      .option('multiline', 'true')\n",
    "      .load('/databricks-datasets/timeseries/Fires/Fire_Department_Calls_for_Service.csv')\n",
    "      .withColumnRenamed('Call Type', 'call_type')\n",
    "      .withColumnRenamed('Received DtTm', 'received')\n",
    "      .withColumnRenamed('Response DtTm', 'responded')\n",
    "      .withColumnRenamed('Neighborhooods - Analysis Boundaries', 'neighborhood')\n",
    "    .select('call_type', 'received', 'responded', 'neighborhood')\n",
    "  )\n",
    "\n",
    "all_tables = []\n",
    "\n",
    "def generate_tables(call_table, response_table, filter):\n",
    "  @dlt.table(\n",
    "    name=call_table,\n",
    "    comment=\"top level tables by call type\"\n",
    "  )\n",
    "  def create_call_table():\n",
    "    return (\n",
    "      spark.sql(\"\"\"\n",
    "        SELECT\n",
    "          unix_timestamp(received,'M/d/yyyy h:m:s a') as ts_received,\n",
    "          unix_timestamp(responded,'M/d/yyyy h:m:s a') as ts_responded,\n",
    "          neighborhood\n",
    "        FROM LIVE.raw_fire_department\n",
    "        WHERE call_type = '{filter}'\n",
    "      \"\"\".format(filter=filter))\n",
    "    )\n",
    "\n",
    "  @dlt.table(\n",
    "    name=response_table,\n",
    "    comment=\"top 10 neighborhoods with fastest response time \"\n",
    "  )\n",
    "  def create_response_table():\n",
    "    return (\n",
    "      spark.sql(\"\"\"\n",
    "        SELECT\n",
    "          neighborhood,\n",
    "          AVG((ts_received - ts_responded)) as response_time\n",
    "        FROM LIVE.{call_table}\n",
    "        GROUP BY 1\n",
    "        ORDER BY response_time\n",
    "        LIMIT 10\n",
    "      \"\"\".format(call_table=call_table))\n",
    "    )\n",
    "\n",
    "  all_tables.append(response_table)\n",
    "\n",
    "generate_tables(\"alarms_table\", \"alarms_response\", \"Alarms\")\n",
    "generate_tables(\"fire_table\", \"fire_response\", \"Structure Fire\")\n",
    "generate_tables(\"medical_table\", \"medical_response\", \"Medical Incident\")\n",
    "\n",
    "@dlt.table(\n",
    "  name=\"best_neighborhoods\",\n",
    "  comment=\"which neighbor appears in the best response time list the most\"\n",
    ")\n",
    "def summary():\n",
    "  target_tables = [dlt.read(t) for t in all_tables]\n",
    "  unioned = functools.reduce(lambda x,y: x.union(y), target_tables)\n",
    "  return (\n",
    "    unioned.groupBy(col(\"neighborhood\"))\n",
    "      .agg(count(\"*\").alias(\"score\"))\n",
    "      .orderBy(desc(\"score\"))\n",
    "  )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Programmatically-create-multiple-tables",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
