{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18e77bce-07c6-4e05-98a9-261ef6292d63",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, from_json, explode, unix_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd692280-219b-41cc-bf6d-751e3932acd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog\", \"<catalog>\", \"Catalog to contain the audit logs\")\n",
    "dbutils.widgets.text(\"database\", \"<schema>\", \"Database, or schema, to contain the audit logs\")\n",
    "dbutils.widgets.text(\"eh_ns_name\", \"<eh_ns_name>\", \"Name of the Event Hubs namespace\")\n",
    "dbutils.widgets.text(\"eh_topic_name\", \"<eh_topic_name>\", \"Name of the Event Hubs topic\")\n",
    "dbutils.widgets.text(\"secret_scope_name\", \"<secret_scope_name>\", \"Name of the secret scope with the Event Hubs access key\")\n",
    "dbutils.widgets.text(\"secret_name\", \"<secret_name>\", \"Name of the secret with Event Hubs access key\")\n",
    "dbutils.widgets.text(\"sink_path\", \"</path/to/checkpoints>\", \"DBFS path to Spark checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17663154-19bb-4f45-ad53-75f48a5da882",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "database = dbutils.widgets.get(\"database\")\n",
    "eh_ns_name = dbutils.widgets.get(\"eh_ns_name\")\n",
    "topic_name = dbutils.widgets.get(\"eh_topic_name\")\n",
    "secret_scope_name = dbutils.widgets.get(\"secret_scope_name\")\n",
    "secret_name = dbutils.widgets.get(\"secret_name\")\n",
    "sink_path = dbutils.widgets.get(\"sink_path\").strip(\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf9b7625-bb51-4e9b-b464-7776279bb522",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {catalog}.{database}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a20873e4-a214-4888-99db-7624704f08e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Event hub configuration\n",
    "connSharedAccessKey = dbutils.secrets.get(secret_scope_name, secret_name)\n",
    "BOOTSTRAP_SERVERS = f\"{eh_ns_name}.servicebus.windows.net:9093\"\n",
    "EH_SASL = f'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"{connSharedAccessKey}\";'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "461057b3-07a1-4d3f-a441-276d2830f09c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_executors = sc._jsc.sc().getExecutorMemoryStatus().size()-1\n",
    "num_cores = sum(sc.parallelize(((\"\")*num_executors), num_executors).mapPartitions(lambda p: [os.cpu_count()]).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa34dd0c-4da1-4f1c-8e7a-6dbbeaaab81f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (spark.readStream\n",
    "      .format(\"kafka\")\n",
    "      .option(\"subscribe\", topic_name)\n",
    "      .option(\"kafka.bootstrap.servers\", BOOTSTRAP_SERVERS)\n",
    "      .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "      .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "      .option(\"kafka.sasl.jaas.config\", EH_SASL)\n",
    "      .option(\"kafka.request.timeout.ms\", \"60000\")\n",
    "      .option(\"kafka.session.timeout.ms\", \"60000\")\n",
    "      .option(\"failOnDataLoss\", \"false\")\n",
    "      .option(\"startingOffsets\", \"earliest\")\n",
    "      .option(\"maxOffsetsPerTrigger\", 100000)\n",
    "      .option(\"minPartitions\", num_cores)\n",
    "      .load()\n",
    "      .withColumn(\"deserializedBody\", col(\"value\").cast(\"string\"))\n",
    "      .withColumn(\"date\", col(\"timestamp\").cast(\"date\"))\n",
    "      .drop(\"value\")\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "611d7f05-f3fb-45f3-a859-5797ff88e525",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_table = f\"{catalog}.{database}.bronze\"\n",
    "checkpoint_path = f\"{sink_path}/checkpoints\"\n",
    "\n",
    "(df\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .partitionBy(\"date\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", f\"{checkpoint_path}/bronze\")\n",
    "    .option(\"mergeSchema\", True)\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(bronze_table)\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1918abf-73ca-429e-ac77-07e036b1913e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "while spark.streams.active != []:\n",
    "    print(\"Waiting for streaming query to finish.\")\n",
    "    time.sleep(5)\n",
    "\n",
    "spark.sql(f\"OPTIMIZE {bronze_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b4720e5-4212-4088-a583-aeb3bb579b41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_schema = (StructType([\n",
    "    StructField(\"records\", ArrayType(StructType([\n",
    "        StructField(\"Host\", StringType()),\n",
    "        StructField(\"category\", StringType()),\n",
    "        StructField(\"identity\", StringType()),\n",
    "        StructField(\"operationName\", StringType()),\n",
    "        StructField(\"operationVersion\", StringType()),\n",
    "        StructField(\"properties\", StructType([\n",
    "            StructField(\"actionName\", StringType()),\n",
    "            StructField(\"logId\", StringType()),\n",
    "            StructField(\"requestId\", StringType()),\n",
    "            StructField(\"requestParams\", StringType()),\n",
    "            StructField(\"response\", StringType()),\n",
    "            StructField(\"serviceName\", StringType()),\n",
    "            StructField(\"sessionId\", StringType()),\n",
    "            StructField(\"sourceIPAddress\", StringType()),\n",
    "            StructField(\"userAgent\", StringType())])),\n",
    "        StructField(\"resourceId\", StringType()),\n",
    "        StructField(\"time\", StringType()),\n",
    "    ])))\n",
    "]))\n",
    "\n",
    "bronzeDF = spark.readStream.table(bronze_table)\n",
    "\n",
    "query = (bronzeDF\n",
    "         .select(\"deserializedBody\")\n",
    "         .withColumn(\"parsedBody\", from_json(\"deserializedBody\", raw_schema))\n",
    "         .select(explode(\"parsedBody.records\").alias(\"streamRecord\"))\n",
    "         .selectExpr(\"streamRecord.*\")\n",
    "         .withColumn(\"version\", col(\"operationVersion\"))\n",
    "         .withColumn(\"date_time\", col(\"time\").cast(\"timestamp\"))\n",
    "         .withColumn(\"timestamp\", unix_timestamp(col(\"date_time\")) * 1000)\n",
    "         .withColumn(\"date\", col(\"time\").cast(\"date\"))\n",
    "         .select(\"category\", \"version\", \"timestamp\", \"date_time\", \"date\", \"properties\", col(\"identity\").alias(\"userIdentity\"))\n",
    "         .selectExpr(\"*\", \"properties.*\")\n",
    "         .withColumnRenamed(\"requestParams\", \"flattened\")\n",
    "         .withColumn(\"identity\", from_json(\"userIdentity\", \"email STRING, subjectName STRING\"))\n",
    "         .withColumn(\"response\", from_json(\"response\", \"errorMessage STRING,result STRING,statusCode BIGINT\"))\n",
    "         .drop(\"properties\", \"userIdentity\")\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bdfa57c-2dbc-46bd-9626-16a9a68a1e65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_table = f\"{catalog}.{database}.silver\"\n",
    "\n",
    "(\n",
    "    query\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .partitionBy(\"date\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", f\"{checkpoint_path}/silver\")\n",
    "    .option(\"mergeSchema\", True)\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(silver_table)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27afe158-b465-4f1c-a9c0-c17299871a98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "while spark.streams.active != []:\n",
    "    print(\"Waiting for streaming query to finish.\")\n",
    "    time.sleep(5)\n",
    "spark.sql(f\"OPTIMIZE {silver_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14b7d2b5-a0d4-40ee-9aeb-3453e9add317",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@udf(StringType())\n",
    "def just_keys_udf(string):\n",
    "    return [i for i in json.loads(string).keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfe17608-4001-47f8-a56f-30f33bb53768",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def flatten_table(service):\n",
    "\n",
    "    service_name = service.replace(\"-\", \"_\")\n",
    "\n",
    "    flattenedStream = spark.readStream.table(silver_table)\n",
    "    flattened = spark.table(silver_table)\n",
    "\n",
    "    schema = StructType()\n",
    "\n",
    "    keys = (\n",
    "        flattened\n",
    "        .filter(col(\"serviceName\") == service_name)\n",
    "        .select(just_keys_udf(col(\"flattened\")).alias(\"keys\"))\n",
    "        .distinct()\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    keysList = [i.asDict()['keys'][1:-1].split(\", \") for i in keys]\n",
    "\n",
    "    keysDistinct = {key for keys in keysList for key in keys if key != \"\"}\n",
    "\n",
    "    if len(keysDistinct) == 0:\n",
    "        schema.add(StructField('placeholder', StringType()))\n",
    "    else:\n",
    "        for key in keysDistinct:\n",
    "            schema.add(StructField(key, StringType()))\n",
    "    # write the df with the correct schema to table\n",
    "    (flattenedStream\n",
    "     .filter(col(\"serviceName\") == service_name)\n",
    "     .withColumn(\"requestParams\", from_json(col(\"flattened\"), schema))\n",
    "     .drop(\"flattened\")\n",
    "     .writeStream\n",
    "     .partitionBy(\"date\")\n",
    "     .outputMode(\"append\")\n",
    "     .format(\"delta\")\n",
    "     .option(\"checkpointLocation\", f\"{checkpoint_path}/gold/{service_name}\")\n",
    "     .option(\"mergeSchema\", True)\n",
    "     .trigger(availableNow=True)\n",
    "     .toTable(f\"{catalog}.{database}.{service_name}\")\n",
    "     )\n",
    "\n",
    "    # optimize the table as well\n",
    "    spark.sql(f\"OPTIMIZE {catalog}.{database}.{service_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc31c79e-1f34-4417-a6a5-845e68765fbb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# For each table name (i.e. event type) create a separate thread and run the ThreadWorker function to save the data to Delta tables.\n",
    "threads = [\n",
    "    threading.Thread(target=flatten_table, args=(service))\n",
    "    for service in spark.table(silver_table).select(\"serviceName\").distinct().collect()\n",
    "]\n",
    "\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fac8b0a-b091-4e50-a763-cba260307f83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "while spark.streams.active != []:\n",
    "    print(\"Waiting for streaming query to finish.\")\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed102914-b272-4a32-b4b1-a861bc4b6a9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SHOW TABLES IN {catalog}.{database}\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "azure-diagnostic-logs-etl-unity-catalog",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
