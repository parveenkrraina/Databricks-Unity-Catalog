{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79debe53-7e32-40cf-b288-83db87654bad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "com.databricks.backend.common.rpc.SparkDriverExceptions$SQLExecutionException: org.apache.spark.sql.catalyst.analysis.NamespaceAlreadyExistsException: [SCHEMA_ALREADY_EXISTS] Cannot create schema `lineagedemo` because it already exists.\n",
       "Choose a different name, drop the existing schema, or add the IF NOT EXISTS clause to tolerate pre-existing schema.\n",
       "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.namespaceAlreadyExistsError(QueryCompilationErrors.scala:1576)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.CreateNamespaceExec.run(CreateNamespaceExec.scala:53)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:47)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:54)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:286)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:286)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:298)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:528)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:225)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1148)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:154)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:477)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:285)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:259)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:280)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:265)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:465)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:69)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:465)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:39)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:339)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:335)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:441)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:265)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:395)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:265)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:217)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:214)\n",
       "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:262)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:123)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1148)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1155)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1155)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:113)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:928)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1148)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:917)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$9(SparkSession.scala:951)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1148)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:951)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:984)\n",
       "\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:695)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$DbClassicStrategy.executeSQLQuery(DriverLocal.scala:237)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSQLSubCommand(DriverLocal.scala:319)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$executeSql$1(DriverLocal.scala:338)\n",
       "\tat scala.collection.immutable.List.map(List.scala:293)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:333)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:709)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$33(DriverLocal.scala:1000)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:45)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:103)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$22(DriverLocal.scala:980)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:69)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:935)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:798)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:790)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:643)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:744)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:520)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:436)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:279)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:376)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:709)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$33(DriverLocal.scala:1000)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:45)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:103)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$22(DriverLocal.scala:980)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:69)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:69)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:935)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:798)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:790)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:643)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:744)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:520)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:436)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:279)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "com.databricks.backend.common.rpc.SparkDriverExceptions$SQLExecutionException: org.apache.spark.sql.catalyst.analysis.NamespaceAlreadyExistsException: [SCHEMA_ALREADY_EXISTS] Cannot create schema `lineagedemo` because it already exists.\nChoose a different name, drop the existing schema, or add the IF NOT EXISTS clause to tolerate pre-existing schema.\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.namespaceAlreadyExistsError(QueryCompilationErrors.scala:1576)\n\tat org.apache.spark.sql.execution.datasources.v2.CreateNamespaceExec.run(CreateNamespaceExec.scala:53)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:47)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:54)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:286)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:286)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:298)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:528)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:225)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1148)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:154)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:477)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:285)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:259)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:280)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:265)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:465)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:465)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:39)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:339)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:335)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:441)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:265)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:395)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:265)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:217)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:214)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:262)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:123)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1148)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1155)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1155)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:113)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:928)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1148)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:917)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$9(SparkSession.scala:951)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1148)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:951)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:984)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:695)\n\tat com.databricks.backend.daemon.driver.DriverLocal$DbClassicStrategy.executeSQLQuery(DriverLocal.scala:237)\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSQLSubCommand(DriverLocal.scala:319)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$executeSql$1(DriverLocal.scala:338)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:333)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:709)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$33(DriverLocal.scala:1000)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:45)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:103)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$22(DriverLocal.scala:980)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:69)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:69)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:935)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:798)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:790)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:643)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:744)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:520)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:436)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:279)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:376)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:709)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$33(DriverLocal.scala:1000)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:45)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:103)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$22(DriverLocal.scala:980)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:69)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:69)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:935)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:798)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:790)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:643)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:744)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:520)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:436)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:279)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorSummary": "NamespaceAlreadyExistsException: [SCHEMA_ALREADY_EXISTS] Cannot create schema `lineagedemo` because it already exists.\nChoose a different name, drop the existing schema, or add the IF NOT EXISTS clause to tolerate pre-existing schema.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE SCHEMA lineage_data.lineagedemo;\n",
    "GRANT USE SCHEMA, CREATE on SCHEMA lineage_data.lineagedemo to `Developers`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53c9c8b4-d975-40cc-9872-e4aa79a4421c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 1
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS\n",
    "  lineage_data.lineagedemo.menu (\n",
    "    recipe_id INT,\n",
    "    app string,\n",
    "    main string,\n",
    "    dessert string\n",
    "  );\n",
    "\n",
    "INSERT INTO lineage_data.lineagedemo.menu\n",
    "    (recipe_id, app, main, dessert)\n",
    "VALUES\n",
    "    (1,\"Ceviche\", \"Tacos\", \"Flan\"),\n",
    "    (2,\"Tomato Soup\", \"Souffle\", \"Creme Brulee\"),\n",
    "    (3,\"Chips\",\"Grilled Cheese\",\"Cheesecake\");\n",
    "\n",
    "CREATE TABLE\n",
    "  lineage_data.lineagedemo.dinner\n",
    "AS SELECT\n",
    "  recipe_id, concat(app,\" + \", main,\" + \",dessert)\n",
    "AS\n",
    "  full_menu\n",
    "FROM\n",
    "  lineage_data.lineagedemo.menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cbcca93-ee97-4b42-abbf-1fb771771859",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import rand, round\n",
    "df = spark.range(3).withColumn(\"price\", round(10*rand(seed=42),2)).withColumnRenamed(\"id\",\"recipe_id\")\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"lineage_data.lineagedemo.price\")\n",
    "\n",
    "dinner = spark.read.table(\"lineage_data.lineagedemo.dinner\")\n",
    "price = spark.read.table(\"lineage_data.lineagedemo.price\")\n",
    "\n",
    "dinner_price = dinner.join(price, on=\"recipe_id\")\n",
    "dinner_price.write.mode(\"overwrite\").saveAsTable(\"lineage_data.lineagedemo.dinner_price\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1514721714968655,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "lineage-example",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
